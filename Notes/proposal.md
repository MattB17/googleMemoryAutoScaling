In large scale compute workloads, resource limits need to be set on the machines running the jobs. This involves setting parameters such as the number of CPU cores and available RAM. Setting values that are too low could cause the job to be killed, while high values could lead to resource wastage. This project will look at the case of RAM and analyze logs of job performance information from google compute clusters managed by Borg. Specifically, this work will analyze traces for a 1 month period in May 2019 to build models that can predict future memory usage of long running jobs based on past usage statistics. Such models would help to automatically manage memory requirements of these jobs, eliminating waste and leading to the jobs being killed less frequently.

The traces group resource usage into time intervals of typically 5 minutes. Data on the amount of memory assigned to the container as well as the maximum memory used at any point during the interval are recorded. Using these statistics, several simple models such as exponential-weighted moving average and autoregressive integrated moving average (ARIMA) models will be used to provide a benchmark for performance. Then several more complex machine learning methods will be tried including: additive boosting, recurrent neural networks, or Markov models. As it is important to have interpretable models in this setting, interpretability and performance will both be taken into account.
